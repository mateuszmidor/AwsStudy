@startmindmap mindmap

title AWS CLI, SDK, IAM, Roles & Policies
  
<style>
mindmapDiagram {
  .encryption {
    BackgroundColor Pink
  }
  .versioning {
    BackgroundColor Violet
  }
  .orange {
    BackgroundColor orange
  }
  .storage {
    BackgroundColor LightSkyBlue
  }
  .cors {
    BackgroundColor LightGreen
  }
}
</style>


*[#Orange] <b>Advanced S3\nSimple Storage Service
 * <b>Multi-part upload/download\n*multipart upload required if >5GB\n*multipart download possible,\ncan request arbitrary range of bytes
 * <b>S3 Select and Glacier Select\n*run simple SQL on CSV/JSON/Apache Parquet files\nfor filtering - to transfer less data\n*supports GZIP and BZIP2 for CSV and JSON
 * <b>Event Notifications\n*S3:ObjectCreated\n*S3:ObjectRemoved\n*S3:Replication\n*allows filtering by object name\n*eg. to generate thumbnails\nof uploaded *.jpg images\nPossible notification targets:\n*SNS\n*SQS\n*Lambda\n*EventBridge\n**forward to 18 AWS services\n**filer by JSON rules\n**forward to multiple targets
 * <b>Storage Classes\nFrom most to least expensive per GB\nNote: can't go back from Glacier;\nneed to copy and restore to Standard <<storage>>
  * Amazon S3 Standard - General Purpose\n*99.99% availability, 11-nines durability\n*for Big Data, mobile & gaming apps, content delivery\n*NO minimum storage duration<<storage>>
  * Amazon S3 Standard - Infrequent Access (IA)\n*99.9% availability, 11-nines durability\n*for backups, disaster recovery\n*minimum storage duration 30 days <<storage>>
  * Amazon S3 One Zone - Infrequent Access (IA)\n*99.5% availability, 11-nines durability in single AZ\n*same as IA but stored in single AZ; AZ loss = data loss\n*for secondary backups and data you can recreate\n*minimum storage duration 30 days <<storage>>
  * Amazon S3 Intelligent Tiering\n*99.9% availability, 11-nines durability\n*automatically moves data between General Purpose and IA\n*allows savings but charges small extra fee for monitoring \n*minimum storage duration 30 days<<storage>>
  * Amazon Glacier Instant Retrieval\n*once a quarter instant retrieval witin milliseconds\n*minimum storage duration 90 days <<storage>>
  * Amazon Glacier Flexible Retrieval\n*11-nines durability\n*stores Archives in Vaults (like Objects in Buckets)\n*very cheap storage, but data retrieval charges fee\n*minimum storage duration 90 days <<storage>>
   * Retrieval options:\n*Expedited 1-5 minutes\n*Standard 3-5 hours\n*Bulk 5-12 hours <<storage>>
  * Amazon Glacier Deep Archive\n*even cheaper option\n*retrieval takes forever :)\n*minimum storage duration 180 days <<storage>>
   * Retrieval options:\n*Standard 12 hours\n*Bulk 48 hours <<storage>>
 * <b>Lifecycle rules\n*Transition actions - eg. move to\nStandard IA 3 weeks after creation,\n to Glacier 3 months later\n*Expiration actions - delete logs\nafter 365 days\n*can be created for specified prefix:\ns3://mybucket/logs/*\n*can be created for specified tags
 * <b>Performance\n*100-200ms response time\nQuotas:\n*3500 PUT, COPY, POST, DELETE\n*5500 GET, HEAD\n*quotas are defined per PREFIX\n*Service Quotas Console to increase\n*S3 Transfer acceleration\n**send data to edge location over public internet\n**forward to actual bucket location over dedicated network\n**compatible with multi-part upload\n**should speed up transfer\n** **bucket name must be DNS-compliant and must not contain periods (".")**
 left side
 * <b>MFA Delete\n*when enabled, permanent object delete (delete version) will require MFA (eg. using AWS CLI)\n*enabling MFA Delete requires AWS CLI cmd from root account\n*aws s3api put-bucket-versioning \ \n  --bucket s3-bucket-mfa-delete-demo \ \n  --versioning-configuration Status=Enabled,MFADelete=Enabled \ \n  --mfa "arn:aws:iam::808768216571:mfa/root-account-mfa-device 975814"\n*delete from CLI should work:\n  aws s3 rm s3://s3-bucket-mfa-delete-demo/beach.jpg 
 * <b>Bucket default encryption\n*used when user uploads file without selecting enctyption type
 * <b>S3 access logs\n*record all requests to S3, both allowed and denied\n*logs are stored in selected Logging S3 Bucket\n*don't monitor the Logging Bucket or it will explode :)
 * <b>Replication (bucket's Management tab)\n*requires versioning enabled in source and destination buckets\n*CRR - Cross Region Replication:\ncompliance, lower latency, replication across acounts\n*SRR - Same Region Replication:\nlog aggregation, live replication between test and prod accounts\n*after enabling only new objects will be replicated\n*to replicate old objects - use Batch Replication\n*can enable replication of DELETE (markers)\n*no replication chaining like:   bucket1 to bucket 2 then bucket 2 to bucket 3
 * <b>Pre-signed URLs for download and upload\nSetup:\n*aws configure set default.s3.signature_version s3v4\nGet download URL:\n*aws s3 presign s3://replication-study-replica-bucket/beach.jpg ------region eu-central-1 --expires-in 15
 * <b>Amazon Athena\n*query service to perform analytics against S3 objects\n*uses standard SQL language to query files\n*supported formats: CSV, JSON, ORC, Avro, Parquet\n*5$ per 1TB scanned data\n*more advanced but more expensive than S3 Select\n*Athena for S3 logs:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/
 @endmindmap